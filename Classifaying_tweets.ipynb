{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo con los tweets pre-clasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"tweets_better_classified.csv\")\n",
    "\n",
    "# Preprocesar las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Tipo'])  # Ajusta 'categoria' a la columna correspondiente\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar el tokenizer BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Funci√≥n para tokenizar los textos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['Texto'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Aplicar el tokenizer a los datos de entrenamiento y prueba\n",
    "train_encodings = tokenizer(list(train['Texto'].values), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test['Texto'].values), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convertir las etiquetas en tensores\n",
    "train_labels = torch.tensor(train['label'].values)\n",
    "test_labels = torch.tensor(test['label'].values)\n",
    "\n",
    "# Crear un dataset personalizado\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n",
    "\n",
    "# Cargar el modelo BERT para clasificaci√≥n de secuencias\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)  \n",
    "\n",
    "# Definir los argumentos del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directorio para guardar los resultados\n",
    "    num_train_epochs=10,              # N√∫mero de √©pocas\n",
    "    per_device_train_batch_size=16,  # Tama√±o del lote en entrenamiento\n",
    "    per_device_eval_batch_size=64,   # Tama√±o del lote en evaluaci√≥n\n",
    "    warmup_steps=500,                # N√∫mero de pasos para warmup\n",
    "    weight_decay=0.01,               # Tasa de decaimiento del peso\n",
    "    logging_dir='./logs',            # Directorio para guardar los logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Crear el Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Reporte de clasificaci√≥n\n",
    "print(classification_report(test_labels, preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./my_model')\n",
    "tokenizer.save_pretrained('./my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificando tweets con BERT pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'tweets_classified.csv' guardado con las etiquetas predichas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cargar el tokenizador y el modelo\n",
    "tokenizer = BertTokenizer.from_pretrained('./my_model')\n",
    "model = BertForSequenceClassification.from_pretrained('./my_model')\n",
    "\n",
    "# Cargar el LabelEncoder (Aseg√∫rate de que es el mismo usado durante el entrenamiento)\n",
    "df_original = pd.read_csv(\"tweets_better_classified.csv\")  # Dataset original para recuperar las etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df_original['Tipo'])  # Ajusta 'Tipo' a la columna de etiquetas original\n",
    "\n",
    "# Cargar el dataset filtrado\n",
    "df_filtered = pd.read_csv(\"tweets_globales_filtrados.csv\")\n",
    "\n",
    "# Define una funci√≥n para predecir nuevos textos\n",
    "def predict_new_text(text):\n",
    "    # Tokenizar el texto\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # Aseg√∫rate de mover el modelo y los inputs a la GPU si est√° disponible\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Hacer la predicci√≥n\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Obtener la clase predicha\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Decodificar la etiqueta predicha\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class])\n",
    "\n",
    "    return predicted_label[0]\n",
    "\n",
    "# Predecir etiquetas para la columna 'texto' y almacenarlas en una nueva columna 'Tipo'\n",
    "df_filtered['Tipo'] = df_filtered['Texto'].apply(predict_new_text)\n",
    "\n",
    "# Guardar el nuevo DataFrame en un archivo CSV\n",
    "df_filtered.to_csv(\"tweets_globales_clasificados.csv\", index=False)\n",
    "\n",
    "print(\"Archivo 'tweets_classified.csv' guardado con las etiquetas predichas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tweets_clean_classified = \"tweets_globales_clasificados.csv\"\n",
    "df_tweets_clean_fil = pd.read_csv(tweets_clean_classified)\n",
    "\n",
    "tipo_counts = df_tweets_clean_fil['Tipo'].value_counts()\n",
    "\n",
    "# Creaci√≥n del histograma\n",
    "tipo_counts.plot(kind='bar', color='skyblue')\n",
    "plt.xlabel('Clasificaci√≥n')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Clasificaciones de los tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets multiclasificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   el√©ctrico  incendio  inundaci√≥n    lluvia  no se relaciona  problema vial  \\\n",
      "0   0.020568  0.022571    0.033255  0.009869          0.01463       0.899107   \n",
      "\n",
      "                                               Texto  \n",
      "0  Fuertes inundaciones accidente a en la avenida...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Cargar el modelo y el tokenizer pre-entrenado\n",
    "model = BertForSequenceClassification.from_pretrained('./my_model4')\n",
    "tokenizer = BertTokenizer.from_pretrained('./my_model4')\n",
    "\n",
    "# Definir un texto de ejemplo\n",
    "texto_ejemplo = \"Fuertes inundaciones accidente a en la avenida garza sada  en Monterrey\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(texto_ejemplo, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Asegurarse de que el modelo est√° en modo de evaluaci√≥n\n",
    "model.eval()\n",
    "\n",
    "# Hacer la predicci√≥n (no se necesita calcular los gradientes)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las logits y calcular las probabilidades con softmax\n",
    "logits = outputs.logits\n",
    "probabilidades = softmax(logits.numpy(), axis=1)[0]\n",
    "\n",
    "# Cargar el LabelEncoder para obtener las categor√≠as\n",
    "df = pd.read_csv(\"df_tweets_procesados.csv\")\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Tipo'])\n",
    "\n",
    "# Obtener los nombres de las categor√≠as\n",
    "categorias = label_encoder.classes_\n",
    "\n",
    "# Crear un nuevo DataFrame con las categor√≠as y las probabilidades\n",
    "df_resultado = pd.DataFrame([probabilidades], columns=categorias)\n",
    "\n",
    "# Agregar el texto de ejemplo para referencia\n",
    "df_resultado['Texto'] = texto_ejemplo\n",
    "\n",
    "# Mostrar el DataFrame con las categor√≠as y sus porcentajes de presencia\n",
    "print(df_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# este es el nuevo clasificador!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\a00832699\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/426 [00:00<?, ?it/s]C:\\Users\\a00832699\\AppData\\Local\\Temp\\ipykernel_14216\\941573390.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "                                                \n",
      " 12%|‚ñà‚ñè        | 50/426 [01:26<10:53,  1.74s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.671, 'grad_norm': 5.880342483520508, 'learning_rate': 1e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 23%|‚ñà‚ñà‚ñé       | 100/426 [02:55<09:46,  1.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3983, 'grad_norm': 5.004861831665039, 'learning_rate': 2e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                                \n",
      " 23%|‚ñà‚ñà‚ñé       | 100/426 [03:14<09:46,  1.80s/it]\n",
      "\u001b[AC:\\Users\\a00832699\\AppData\\Local\\Temp\\ipykernel_14216\\941573390.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1365094184875488, 'eval_accuracy': 0.6707964601769911, 'eval_runtime': 18.3804, 'eval_samples_per_second': 30.739, 'eval_steps_per_second': 0.979, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 150/426 [04:42<08:29,  1.85s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9411, 'grad_norm': 4.041423797607422, 'learning_rate': 1.6932515337423315e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 200/426 [06:47<15:23,  4.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7253, 'grad_norm': 3.585031747817993, 'learning_rate': 1.3865030674846627e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                                \n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 200/426 [07:29<15:23,  4.09s/it]\n",
      "\u001b[AC:\\Users\\a00832699\\AppData\\Local\\Temp\\ipykernel_14216\\941573390.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5354270935058594, 'eval_accuracy': 0.8407079646017699, 'eval_runtime': 42.0585, 'eval_samples_per_second': 13.434, 'eval_steps_per_second': 0.428, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 250/426 [09:43<05:01,  1.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5355, 'grad_norm': 6.749011993408203, 'learning_rate': 1.079754601226994e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 300/426 [11:09<03:43,  1.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.413, 'grad_norm': 4.840836524963379, 'learning_rate': 7.730061349693252e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                                \n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 300/426 [11:28<03:43,  1.77s/it]\n",
      "\u001b[AC:\\Users\\a00832699\\AppData\\Local\\Temp\\ipykernel_14216\\941573390.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3067410886287689, 'eval_accuracy': 0.9185840707964602, 'eval_runtime': 18.3101, 'eval_samples_per_second': 30.857, 'eval_steps_per_second': 0.983, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 350/426 [12:57<02:16,  1.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2877, 'grad_norm': 1.0295826196670532, 'learning_rate': 4.662576687116564e-06, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 400/426 [14:26<00:46,  1.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2689, 'grad_norm': 4.481869220733643, 'learning_rate': 1.5950920245398775e-06, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                 \n",
      "\u001b[A                                                \n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 400/426 [14:45<00:46,  1.79s/it]\n",
      "\u001b[AC:\\Users\\a00832699\\AppData\\Local\\Temp\\ipykernel_14216\\941573390.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24593138694763184, 'eval_accuracy': 0.9309734513274336, 'eval_runtime': 18.4244, 'eval_samples_per_second': 30.666, 'eval_steps_per_second': 0.977, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [15:31<00:00,  2.19s/it]  \n",
      "C:\\Users\\a00832699\\AppData\\Local\\Temp\\ipykernel_14216\\941573390.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 931.6598, 'train_samples_per_second': 7.277, 'train_steps_per_second': 0.457, 'train_loss': 0.7483531645206218, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:17<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      el√©ctrico       0.87      0.85      0.86        62\n",
      "       incendio       0.94      0.94      0.94        47\n",
      "     inundaci√≥n       0.93      1.00      0.97        57\n",
      "         lluvia       1.00      0.98      0.99       247\n",
      "no se relaciona       0.80      0.80      0.80        79\n",
      "  problema vial       0.92      0.95      0.93        73\n",
      "\n",
      "       accuracy                           0.93       565\n",
      "      macro avg       0.91      0.92      0.91       565\n",
      "   weighted avg       0.93      0.93      0.93       565\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./my_model6\\\\tokenizer_config.json',\n",
       " './my_model6\\\\special_tokens_map.json',\n",
       " './my_model6\\\\vocab.txt',\n",
       " './my_model6\\\\added_tokens.json')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import classification_report\n",
    "import evaluate\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"df_balanceado.csv\")\n",
    "\n",
    "# Preprocesar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Tipo'])  \n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizar los textos (Aseg√∫rate de que son strings)\n",
    "train_encodings = tokenizer(train['Texto'].astype(str).tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test['Texto'].astype(str).tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convertir las etiquetas a tensores\n",
    "train_labels = torch.tensor(train['label'].values)\n",
    "test_labels = torch.tensor(test['label'].values)\n",
    "\n",
    "# Crear dataset personalizado para el entrenamiento\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]) \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n",
    "\n",
    "# Cargar el modelo preentrenado de BERT para clasificaci√≥n de secuencias\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Cargar la m√©trica de accuracy desde la nueva biblioteca evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"]}\n",
    "\n",
    "# Definir los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.03,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  \n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Crear el Trainer para el modelo\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics \n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] \n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1)\n",
    "preds = torch.argmax(pred_probs, dim=1).numpy()\n",
    "\n",
    "# Imprimir reporte de clasificaci√≥n\n",
    "print(classification_report(test_labels.numpy(), preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Guardar el modelo y el tokenizer\n",
    "model.save_pretrained('./my_model6')\n",
    "tokenizer.save_pretrained('./my_model6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Recuperar los registros de entrenamiento y evaluaci√≥n\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Filtrar los valores de accuracy y epochs de los logs\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m training_logs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]  \u001b[38;5;66;03m# Precisi√≥n del conjunto de entrenamiento\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recuperar los registros de entrenamiento y evaluaci√≥n\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "# Filtrar los valores de accuracy y epochs de los logs\n",
    "train_accuracy = [log['accuracy'] for log in training_logs if 'accuracy' in log]  # Precisi√≥n del conjunto de entrenamiento\n",
    "eval_accuracy = [log['eval_accuracy'] for log in training_logs if 'eval_accuracy' in log]  # Precisi√≥n del conjunto de evaluaci√≥n\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "\n",
    "# Graficar accuracy de entrenamiento y validaci√≥n\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, train_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, eval_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Cargar el modelo y el tokenizer pre-entrenado\n",
    "model = BertForSequenceClassification.from_pretrained('./my_model6')\n",
    "tokenizer = BertTokenizer.from_pretrained('./my_model6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   el√©ctrico  incendio  inundaci√≥n    lluvia  no se relaciona  problema vial  \\\n",
      "0     0.0273   0.02136    0.522385  0.030191         0.306181       0.092583   \n",
      "\n",
      "                                               Texto  \n",
      "0  Av Los √Ångeles  a la altura de Churubusco en S...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir un texto de ejemplo\n",
    "texto_ejemplo = \"Av Los √Ångeles  a la altura de Churubusco en San Nicol√°s de los Garza s√∫per inundado Cuidado Tomen sus precauciones\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(texto_ejemplo, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Asegurarse de que el modelo est√° en modo de evaluaci√≥n\n",
    "model.eval()\n",
    "\n",
    "# Hacer la predicci√≥n (no se necesita calcular los gradientes)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtener las logits y calcular las probabilidades con softmax\n",
    "logits = outputs.logits\n",
    "probabilidades = softmax(logits.numpy(), axis=1)[0]\n",
    "\n",
    "# Cargar el LabelEncoder para obtener las categor√≠as\n",
    "df = pd.read_csv(\"df_tweets_procesados.csv\")\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Tipo'])\n",
    "\n",
    "# Obtener los nombres de las categor√≠as\n",
    "categorias = label_encoder.classes_\n",
    "\n",
    "# Crear un nuevo DataFrame con las categor√≠as y las probabilidades\n",
    "df_resultado = pd.DataFrame([probabilidades], columns=categorias)\n",
    "\n",
    "# Agregar el texto de ejemplo para referencia\n",
    "df_resultado['Texto'] = texto_ejemplo\n",
    "\n",
    "# Mostrar el DataFrame con las categor√≠as y sus porcentajes de presencia\n",
    "print(df_resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tercera arquitectura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A esta arquitectura se le aplic√≥ un submuestreo a la categor√≠a de lluvia, debido a que esta categor√≠a era muy dominante. A dem√°s se le aplic√≥ un early stopping para evitar el overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.20k/4.20k [00:00<00:00, 2.40MB/s]\n",
      "/Users/Daniel/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/174 [00:00<?, ?it/s]/var/folders/8b/d34q145x44x2xf_06_gchyfw0000gq/T/ipykernel_28759/835213133.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n",
      "  4%|‚ñç         | 7/174 [02:55<59:43, 21.46s/it]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 103\u001b[0m\n\u001b[1;32m     93\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     94\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     95\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)] \n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo\u001b[39;00m\n\u001b[1;32m    106\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/accelerate/accelerator.py:2159\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2159\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import classification_report\n",
    "import evaluate\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"df_balanceado.csv\")\n",
    "\n",
    "# Preprocesar etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['Tipo'])  \n",
    "\n",
    "# Submuestreo de la clase dominante (lluvia)\n",
    "X = df['Texto'].values.reshape(-1, 1)  # Reshape porque RandomUnderSampler espera m√°s de una columna en X\n",
    "y = df['label']\n",
    "\n",
    "# Definir el submuestreo para balancear las clases\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = under_sampler.fit_resample(X, y)\n",
    "\n",
    "# Convertir a dataframe para proceder\n",
    "df_resampled = pd.DataFrame({'Texto': X_resampled.flatten(), 'label': y_resampled})\n",
    "\n",
    "# Dividir el dataset balanceado en entrenamiento y prueba\n",
    "train, test = train_test_split(df_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inicializar el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenizar los textos (Aseg√∫rate de que son strings)\n",
    "train_encodings = tokenizer(train['Texto'].astype(str).tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test['Texto'].astype(str).tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convertir las etiquetas a tensores\n",
    "train_labels = torch.tensor(train['label'].values)\n",
    "test_labels = torch.tensor(test['label'].values)\n",
    "\n",
    "# Crear dataset personalizado para el entrenamiento\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]) \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)\n",
    "\n",
    "# Cargar el modelo preentrenado de BERT para clasificaci√≥n de secuencias\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Cargar la m√©trica de accuracy desde la nueva biblioteca evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"]}\n",
    "\n",
    "# Definir los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.03,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  \n",
    "    greater_is_better=True,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Crear el Trainer para el modelo\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] \n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1)\n",
    "preds = torch.argmax(pred_probs, dim=1).numpy()\n",
    "\n",
    "# Imprimir reporte de clasificaci√≥n\n",
    "print(classification_report(test_labels.numpy(), preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Guardar el modelo y el tokenizer\n",
    "model.save_pretrained('./my_model7')\n",
    "tokenizer.save_pretrained('./my_model7')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
